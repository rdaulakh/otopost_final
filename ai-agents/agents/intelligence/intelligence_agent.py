import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import json
import pandas as pd
import numpy as np
from dataclasses import dataclass

from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import BaseTool, tool
from langchain.schema import BaseMessage, HumanMessage, SystemMessage
from pydantic import BaseModel, Field

from agents.base_agent import BaseAgent, AgentTask, AgentResponse
from config.settings import AgentType, get_platform_config
from memory.chroma_manager import chroma_manager
from utils.logger import get_agent_logger, log_analytics_insight

@dataclass
class DataInsight:
    """Represents a data insight generated by the Intelligence Agent."""
    type: str
    title: str
    description: str
    confidence: float
    impact: str  # high, medium, low
    recommendation: str
    data_points: Dict[str, Any]
    created_at: datetime

class TrendAnalysisInput(BaseModel):
    """Input schema for trend analysis."""
    platform: str = Field(description="Social media platform to analyze")
    timeframe: str = Field(description="Time period for analysis (7d, 30d, 90d)")
    metrics: List[str] = Field(description="Metrics to analyze")

class CompetitorAnalysisInput(BaseModel):
    """Input schema for competitor analysis."""
    competitors: List[str] = Field(description="List of competitor handles/names")
    platform: str = Field(description="Platform to analyze")
    analysis_type: str = Field(description="Type of analysis (content, engagement, strategy)")

class AudienceInsightInput(BaseModel):
    """Input schema for audience insights."""
    platform: str = Field(description="Platform to analyze")
    segment: Optional[str] = Field(description="Audience segment to focus on")
    metrics: List[str] = Field(description="Metrics to analyze")

class IntelligenceAgent(BaseAgent):
    """Intelligence Agent for data analysis and insights generation."""
    
    def __init__(self, organization_id: str):
        super().__init__(AgentType.INTELLIGENCE, organization_id)
        self.insights_cache = {}
        self.analysis_history = []
        
    async def _create_agent_prompt(self) -> ChatPromptTemplate:
        """Create the Intelligence Agent's system prompt."""
        system_prompt = """You are an AI Intelligence Agent specialized in social media data analysis and insights generation.

Your primary responsibilities:
1. Analyze social media performance data and identify trends
2. Generate actionable insights from complex datasets
3. Perform competitive analysis and benchmarking
4. Identify audience behavior patterns and preferences
5. Predict future performance based on historical data
6. Provide data-driven recommendations for strategy optimization

Key capabilities:
- Advanced statistical analysis and pattern recognition
- Trend identification and forecasting
- Competitive intelligence gathering
- Audience segmentation and behavior analysis
- Performance correlation analysis
- Anomaly detection in social media metrics

Guidelines:
- Always provide confidence scores for your insights (0.0 to 1.0)
- Include specific data points to support your conclusions
- Categorize impact as high, medium, or low
- Provide actionable recommendations based on insights
- Consider platform-specific nuances in your analysis
- Use statistical significance when making claims
- Identify both opportunities and risks in the data

When analyzing data:
1. Look for patterns, trends, and anomalies
2. Compare performance across different time periods
3. Identify top-performing content characteristics
4. Analyze audience engagement patterns
5. Benchmark against industry standards
6. Provide predictive insights when possible

Remember: Your insights should be data-driven, actionable, and clearly communicated to help optimize social media strategy."""

        return ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])
    
    async def _initialize_tools(self) -> List[BaseTool]:
        """Initialize Intelligence Agent specific tools."""
        
        @tool
        def analyze_performance_trends(data: str) -> str:
            """Analyze performance trends from social media data."""
            try:
                # Parse the input data
                data_dict = json.loads(data) if isinstance(data, str) else data
                
                # Perform trend analysis
                insights = []
                
                # Example trend analysis logic
                if 'engagement_rate' in data_dict:
                    engagement_data = data_dict['engagement_rate']
                    if isinstance(engagement_data, list) and len(engagement_data) > 1:
                        trend = "increasing" if engagement_data[-1] > engagement_data[0] else "decreasing"
                        change = abs(engagement_data[-1] - engagement_data[0])
                        insights.append(f"Engagement rate is {trend} with a change of {change:.2%}")
                
                if 'reach' in data_dict:
                    reach_data = data_dict['reach']
                    if isinstance(reach_data, list) and len(reach_data) > 1:
                        avg_reach = sum(reach_data) / len(reach_data)
                        insights.append(f"Average reach is {avg_reach:,.0f}")
                
                return json.dumps({
                    "insights": insights,
                    "confidence": 0.8,
                    "analysis_type": "performance_trends"
                })
                
            except Exception as e:
                return json.dumps({"error": str(e), "confidence": 0.0})
        
        @tool
        def identify_content_patterns(content_data: str) -> str:
            """Identify patterns in high-performing content."""
            try:
                data_dict = json.loads(content_data) if isinstance(content_data, str) else content_data
                
                patterns = []
                
                # Analyze content characteristics
                if 'posts' in data_dict:
                    posts = data_dict['posts']
                    
                    # Analyze post types
                    post_types = {}
                    for post in posts:
                        post_type = post.get('type', 'unknown')
                        engagement = post.get('engagement', 0)
                        if post_type not in post_types:
                            post_types[post_type] = []
                        post_types[post_type].append(engagement)
                    
                    # Find best performing post type
                    best_type = max(post_types.keys(), 
                                  key=lambda x: sum(post_types[x]) / len(post_types[x]))
                    avg_engagement = sum(post_types[best_type]) / len(post_types[best_type])
                    
                    patterns.append(f"Best performing post type: {best_type} with average engagement of {avg_engagement:.0f}")
                    
                    # Analyze posting times
                    time_performance = {}
                    for post in posts:
                        hour = post.get('posted_at', '12:00').split(':')[0]
                        engagement = post.get('engagement', 0)
                        if hour not in time_performance:
                            time_performance[hour] = []
                        time_performance[hour].append(engagement)
                    
                    if time_performance:
                        best_hour = max(time_performance.keys(),
                                      key=lambda x: sum(time_performance[x]) / len(time_performance[x]))
                        patterns.append(f"Best posting time: {best_hour}:00 hour")
                
                return json.dumps({
                    "patterns": patterns,
                    "confidence": 0.75,
                    "analysis_type": "content_patterns"
                })
                
            except Exception as e:
                return json.dumps({"error": str(e), "confidence": 0.0})
        
        @tool
        def analyze_audience_behavior(audience_data: str) -> str:
            """Analyze audience behavior and preferences."""
            try:
                data_dict = json.loads(audience_data) if isinstance(audience_data, str) else audience_data
                
                behaviors = []
                
                # Analyze engagement patterns
                if 'engagement_by_time' in data_dict:
                    time_data = data_dict['engagement_by_time']
                    peak_times = sorted(time_data.items(), key=lambda x: x[1], reverse=True)[:3]
                    behaviors.append(f"Peak engagement times: {', '.join([f'{time}:00' for time, _ in peak_times])}")
                
                # Analyze content preferences
                if 'content_preferences' in data_dict:
                    prefs = data_dict['content_preferences']
                    top_pref = max(prefs.items(), key=lambda x: x[1])
                    behaviors.append(f"Most preferred content type: {top_pref[0]} ({top_pref[1]:.1%} preference)")
                
                # Analyze demographics
                if 'demographics' in data_dict:
                    demo = data_dict['demographics']
                    if 'age_groups' in demo:
                        top_age = max(demo['age_groups'].items(), key=lambda x: x[1])
                        behaviors.append(f"Primary age group: {top_age[0]} ({top_age[1]:.1%})")
                
                return json.dumps({
                    "behaviors": behaviors,
                    "confidence": 0.7,
                    "analysis_type": "audience_behavior"
                })
                
            except Exception as e:
                return json.dumps({"error": str(e), "confidence": 0.0})
        
        @tool
        def competitive_analysis(competitor_data: str) -> str:
            """Perform competitive analysis and benchmarking."""
            try:
                data_dict = json.loads(competitor_data) if isinstance(competitor_data, str) else competitor_data
                
                analysis = []
                
                if 'competitors' in data_dict:
                    competitors = data_dict['competitors']
                    
                    # Compare engagement rates
                    our_engagement = data_dict.get('our_engagement_rate', 0)
                    competitor_engagements = [comp.get('engagement_rate', 0) for comp in competitors]
                    
                    if competitor_engagements:
                        avg_competitor = sum(competitor_engagements) / len(competitor_engagements)
                        performance = "above" if our_engagement > avg_competitor else "below"
                        difference = abs(our_engagement - avg_competitor)
                        
                        analysis.append(f"Our engagement rate ({our_engagement:.2%}) is {performance} competitor average ({avg_competitor:.2%}) by {difference:.2%}")
                    
                    # Analyze posting frequency
                    our_frequency = data_dict.get('our_posting_frequency', 0)
                    competitor_frequencies = [comp.get('posting_frequency', 0) for comp in competitors]
                    
                    if competitor_frequencies:
                        avg_freq = sum(competitor_frequencies) / len(competitor_frequencies)
                        analysis.append(f"Our posting frequency: {our_frequency} posts/week vs competitor average: {avg_freq:.1f}")
                    
                    # Content strategy comparison
                    if 'content_strategies' in data_dict:
                        strategies = data_dict['content_strategies']
                        common_strategies = [strategy for strategy in strategies if strategies[strategy] > 0.5]
                        analysis.append(f"Common competitor strategies: {', '.join(common_strategies)}")
                
                return json.dumps({
                    "analysis": analysis,
                    "confidence": 0.8,
                    "analysis_type": "competitive_analysis"
                })
                
            except Exception as e:
                return json.dumps({"error": str(e), "confidence": 0.0})
        
        @tool
        def predict_performance(historical_data: str) -> str:
            """Predict future performance based on historical data."""
            try:
                data_dict = json.loads(historical_data) if isinstance(historical_data, str) else historical_data
                
                predictions = []
                
                # Simple trend-based prediction
                if 'metrics' in data_dict:
                    for metric, values in data_dict['metrics'].items():
                        if len(values) >= 3:
                            # Calculate trend
                            recent_avg = sum(values[-3:]) / 3
                            older_avg = sum(values[:-3]) / len(values[:-3]) if len(values) > 3 else recent_avg
                            
                            trend = (recent_avg - older_avg) / older_avg if older_avg != 0 else 0
                            predicted_value = recent_avg * (1 + trend)
                            
                            predictions.append({
                                "metric": metric,
                                "current_value": values[-1],
                                "predicted_value": predicted_value,
                                "trend": "increasing" if trend > 0 else "decreasing",
                                "confidence": min(0.9, 0.5 + abs(trend))
                            })
                
                return json.dumps({
                    "predictions": predictions,
                    "confidence": 0.7,
                    "analysis_type": "performance_prediction"
                })
                
            except Exception as e:
                return json.dumps({"error": str(e), "confidence": 0.0})
        
        @tool
        def detect_anomalies(data: str) -> str:
            """Detect anomalies in social media metrics."""
            try:
                data_dict = json.loads(data) if isinstance(data, str) else data
                
                anomalies = []
                
                if 'time_series' in data_dict:
                    for metric, values in data_dict['time_series'].items():
                        if len(values) >= 5:
                            # Simple anomaly detection using standard deviation
                            mean_val = sum(values) / len(values)
                            std_dev = (sum((x - mean_val) ** 2 for x in values) / len(values)) ** 0.5
                            
                            for i, value in enumerate(values):
                                if abs(value - mean_val) > 2 * std_dev:
                                    anomalies.append({
                                        "metric": metric,
                                        "index": i,
                                        "value": value,
                                        "expected_range": [mean_val - 2*std_dev, mean_val + 2*std_dev],
                                        "severity": "high" if abs(value - mean_val) > 3 * std_dev else "medium"
                                    })
                
                return json.dumps({
                    "anomalies": anomalies,
                    "confidence": 0.8,
                    "analysis_type": "anomaly_detection"
                })
                
            except Exception as e:
                return json.dumps({"error": str(e), "confidence": 0.0})
        
        return [
            analyze_performance_trends,
            identify_content_patterns,
            analyze_audience_behavior,
            competitive_analysis,
            predict_performance,
            detect_anomalies
        ]
    
    async def _process_task(self, task: AgentTask) -> Any:
        """Process Intelligence Agent specific tasks."""
        task_type = task.type
        input_data = task.input_data
        
        try:
            if task_type == "trend_analysis":
                return await self._analyze_trends(input_data)
            elif task_type == "competitor_analysis":
                return await self._analyze_competitors(input_data)
            elif task_type == "audience_insights":
                return await self._generate_audience_insights(input_data)
            elif task_type == "performance_prediction":
                return await self._predict_performance(input_data)
            elif task_type == "anomaly_detection":
                return await self._detect_anomalies(input_data)
            elif task_type == "comprehensive_analysis":
                return await self._comprehensive_analysis(input_data)
            else:
                # Use the agent executor for general intelligence tasks
                result = await self.agent_executor.ainvoke({
                    "input": f"Analyze the following data and provide insights: {json.dumps(input_data)}"
                })
                return result["output"]
                
        except Exception as e:
            self.logger.error(f"Error processing task {task_type}: {str(e)}")
            raise
    
    async def _analyze_trends(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze performance trends."""
        platform = input_data.get('platform', 'all')
        timeframe = input_data.get('timeframe', '30d')
        metrics = input_data.get('metrics', ['engagement', 'reach', 'impressions'])
        
        # Use the agent executor to analyze trends
        analysis_prompt = f"""
        Analyze the following social media performance data for trends:
        Platform: {platform}
        Timeframe: {timeframe}
        Metrics to analyze: {', '.join(metrics)}
        Data: {json.dumps(input_data.get('data', {}))}
        
        Provide:
        1. Key trends identified
        2. Performance changes over time
        3. Seasonal patterns if any
        4. Recommendations based on trends
        """
        
        result = await self.agent_executor.ainvoke({"input": analysis_prompt})
        
        # Parse and structure the result
        insights = self._parse_analysis_result(result["output"], "trend_analysis")
        
        # Store insights in memory
        await self._store_insights(insights)
        
        return {
            "analysis_type": "trend_analysis",
            "platform": platform,
            "timeframe": timeframe,
            "insights": insights,
            "confidence": self._calculate_analysis_confidence(insights),
            "generated_at": datetime.utcnow().isoformat()
        }
    
    async def _analyze_competitors(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Perform competitor analysis."""
        competitors = input_data.get('competitors', [])
        platform = input_data.get('platform', 'all')
        analysis_type = input_data.get('analysis_type', 'comprehensive')
        
        analysis_prompt = f"""
        Perform competitive analysis with the following parameters:
        Competitors: {', '.join(competitors)}
        Platform: {platform}
        Analysis Type: {analysis_type}
        Our Data: {json.dumps(input_data.get('our_data', {}))}
        Competitor Data: {json.dumps(input_data.get('competitor_data', {}))}
        
        Provide:
        1. Performance comparison
        2. Content strategy differences
        3. Engagement rate analysis
        4. Opportunities identified
        5. Competitive advantages/disadvantages
        """
        
        result = await self.agent_executor.ainvoke({"input": analysis_prompt})
        
        insights = self._parse_analysis_result(result["output"], "competitor_analysis")
        await self._store_insights(insights)
        
        return {
            "analysis_type": "competitor_analysis",
            "competitors": competitors,
            "platform": platform,
            "insights": insights,
            "confidence": self._calculate_analysis_confidence(insights),
            "generated_at": datetime.utcnow().isoformat()
        }
    
    async def _generate_audience_insights(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate audience behavior insights."""
        platform = input_data.get('platform', 'all')
        segment = input_data.get('segment', 'all')
        
        analysis_prompt = f"""
        Analyze audience behavior and generate insights:
        Platform: {platform}
        Segment: {segment}
        Audience Data: {json.dumps(input_data.get('audience_data', {}))}
        Engagement Data: {json.dumps(input_data.get('engagement_data', {}))}
        
        Provide:
        1. Audience behavior patterns
        2. Content preferences
        3. Optimal posting times
        4. Engagement triggers
        5. Demographic insights
        """
        
        result = await self.agent_executor.ainvoke({"input": analysis_prompt})
        
        insights = self._parse_analysis_result(result["output"], "audience_insights")
        await self._store_insights(insights)
        
        return {
            "analysis_type": "audience_insights",
            "platform": platform,
            "segment": segment,
            "insights": insights,
            "confidence": self._calculate_analysis_confidence(insights),
            "generated_at": datetime.utcnow().isoformat()
        }
    
    async def _predict_performance(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Predict future performance based on historical data."""
        historical_data = input_data.get('historical_data', {})
        prediction_period = input_data.get('prediction_period', '30d')
        
        analysis_prompt = f"""
        Predict future social media performance based on historical data:
        Historical Data: {json.dumps(historical_data)}
        Prediction Period: {prediction_period}
        
        Provide:
        1. Performance predictions for key metrics
        2. Confidence levels for predictions
        3. Factors influencing predictions
        4. Recommended actions to improve predicted performance
        5. Risk factors to monitor
        """
        
        result = await self.agent_executor.ainvoke({"input": analysis_prompt})
        
        insights = self._parse_analysis_result(result["output"], "performance_prediction")
        await self._store_insights(insights)
        
        return {
            "analysis_type": "performance_prediction",
            "prediction_period": prediction_period,
            "insights": insights,
            "confidence": self._calculate_analysis_confidence(insights),
            "generated_at": datetime.utcnow().isoformat()
        }
    
    async def _detect_anomalies(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Detect anomalies in social media metrics."""
        metrics_data = input_data.get('metrics_data', {})
        sensitivity = input_data.get('sensitivity', 'medium')
        
        analysis_prompt = f"""
        Detect anomalies in social media metrics:
        Metrics Data: {json.dumps(metrics_data)}
        Sensitivity Level: {sensitivity}
        
        Identify:
        1. Unusual spikes or drops in metrics
        2. Patterns that deviate from normal behavior
        3. Potential causes for anomalies
        4. Impact assessment of detected anomalies
        5. Recommended actions to address anomalies
        """
        
        result = await self.agent_executor.ainvoke({"input": analysis_prompt})
        
        insights = self._parse_analysis_result(result["output"], "anomaly_detection")
        await self._store_insights(insights)
        
        return {
            "analysis_type": "anomaly_detection",
            "sensitivity": sensitivity,
            "insights": insights,
            "confidence": self._calculate_analysis_confidence(insights),
            "generated_at": datetime.utcnow().isoformat()
        }
    
    async def _comprehensive_analysis(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Perform comprehensive analysis combining multiple analysis types."""
        analysis_prompt = f"""
        Perform a comprehensive social media intelligence analysis:
        Data: {json.dumps(input_data)}
        
        Provide a complete analysis including:
        1. Performance trends and patterns
        2. Audience behavior insights
        3. Content performance analysis
        4. Competitive positioning
        5. Anomaly detection
        6. Future performance predictions
        7. Strategic recommendations
        8. Risk assessment
        """
        
        result = await self.agent_executor.ainvoke({"input": analysis_prompt})
        
        insights = self._parse_analysis_result(result["output"], "comprehensive_analysis")
        await self._store_insights(insights)
        
        return {
            "analysis_type": "comprehensive_analysis",
            "insights": insights,
            "confidence": self._calculate_analysis_confidence(insights),
            "generated_at": datetime.utcnow().isoformat()
        }
    
    def _parse_analysis_result(self, result: str, analysis_type: str) -> List[DataInsight]:
        """Parse analysis result into structured insights."""
        insights = []
        
        # Simple parsing logic (in production, this would be more sophisticated)
        lines = result.split('\n')
        current_insight = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Look for insight indicators
            if any(keyword in line.lower() for keyword in ['insight:', 'trend:', 'pattern:', 'recommendation:']):
                if current_insight:
                    insights.append(current_insight)
                
                current_insight = DataInsight(
                    type=analysis_type,
                    title=line,
                    description="",
                    confidence=0.7,  # Default confidence
                    impact="medium",  # Default impact
                    recommendation="",
                    data_points={},
                    created_at=datetime.utcnow()
                )
            elif current_insight:
                current_insight.description += line + " "
        
        if current_insight:
            insights.append(current_insight)
        
        # If no structured insights found, create a general one
        if not insights:
            insights.append(DataInsight(
                type=analysis_type,
                title=f"{analysis_type.replace('_', ' ').title()} Results",
                description=result,
                confidence=0.6,
                impact="medium",
                recommendation="Review the analysis and implement suggested improvements",
                data_points={},
                created_at=datetime.utcnow()
            ))
        
        return insights
    
    def _calculate_analysis_confidence(self, insights: List[DataInsight]) -> float:
        """Calculate overall confidence for the analysis."""
        if not insights:
            return 0.0
        
        return sum(insight.confidence for insight in insights) / len(insights)
    
    async def _store_insights(self, insights: List[DataInsight]):
        """Store insights in memory for future reference."""
        for insight in insights:
            try:
                # Store in Chroma memory
                await chroma_manager.store_knowledge(
                    self.organization_id,
                    f"{insight.title}: {insight.description}",
                    insight.type,
                    "intelligence",
                    "intelligence_agent",
                    insight.confidence,
                    {
                        "impact": insight.impact,
                        "recommendation": insight.recommendation,
                        "data_points": insight.data_points
                    }
                )
                
                # Log the insight
                log_analytics_insight(
                    insight.type,
                    insight.title,
                    insight.confidence,
                    self.agent_type.value,
                    organization_id=self.organization_id,
                    impact=insight.impact
                )
                
            except Exception as e:
                self.logger.warning(f"Failed to store insight: {str(e)}")
    
    async def get_cached_insights(self, analysis_type: str, max_age_hours: int = 24) -> List[Dict[str, Any]]:
        """Get cached insights from memory."""
        try:
            cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)
            
            # Search for recent insights
            insights = await chroma_manager.search_knowledge(
                self.organization_id,
                analysis_type,
                "intelligence",
                limit=10,
                confidence_threshold=0.5
            )
            
            # Filter by age
            recent_insights = []
            for insight in insights:
                created_at = datetime.fromisoformat(insight['metadata'].get('created_at', '1970-01-01'))
                if created_at > cutoff_time:
                    recent_insights.append(insight)
            
            return recent_insights
            
        except Exception as e:
            self.logger.warning(f"Failed to get cached insights: {str(e)}")
            return []
    
    def _calculate_confidence(self, result: Any) -> float:
        """Calculate confidence score for Intelligence Agent results."""
        if not result:
            return 0.0
        
        if isinstance(result, dict):
            # Check for analysis completeness
            confidence_factors = []
            
            if result.get('insights'):
                confidence_factors.append(0.3)  # Has insights
            
            if result.get('analysis_type'):
                confidence_factors.append(0.2)  # Has analysis type
            
            if result.get('confidence'):
                confidence_factors.append(result['confidence'] * 0.5)  # Use provided confidence
            
            return min(sum(confidence_factors), 1.0)
        
        return 0.6  # Default confidence for other result types

